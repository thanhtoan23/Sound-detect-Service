"""
Audio Classifier Module
Ph√¢n lo·∫°i lo·∫°i √¢m thanh: Speech, Music, Noise, Silence

S·ª≠ d·ª•ng:
- PyAudio ƒë·ªÉ thu √¢m t·ª´ ReSpeaker
- Numpy/Scipy ƒë·ªÉ ph√¢n t√≠ch t√≠n hi·ªáu
- C√°c ƒë·∫∑c tr∆∞ng: Volume, Zero-Crossing Rate, Spectral features
"""

import pyaudio
import numpy as np
import wave
import time
from scipy import signal
from typing import Dict, List, Optional
from enum import Enum


class SoundType(Enum):
    """C√°c lo·∫°i √¢m thanh"""
    SILENCE = "silence"
    SPEECH = "speech"
    MUSIC = "music"
    NOISE = "noise"
    UNKNOWN = "unknown"


class AudioClassifier:
    """
    Ph√¢n lo·∫°i √¢m thanh d·ª±a tr√™n ƒë·∫∑c tr∆∞ng t√≠n hi·ªáu
    """
    
    # C·∫•u h√¨nh PyAudio
    CHUNK = 1024  # S·ªë samples m·ªói frame
    RATE = 16000  # Sample rate (Hz)
    CHANNELS = 6  # ReSpeaker c√≥ 6 channels (6_channels_firmware)
    FORMAT = pyaudio.paInt16
    
    # Ng∆∞·ª°ng ph√¢n lo·∫°i
    SILENCE_THRESHOLD = 500  # RMS threshold cho silence
    SPEECH_ZCR_MIN = 0.01    # Zero-crossing rate min cho speech
    SPEECH_ZCR_MAX = 0.15    # Zero-crossing rate max cho speech
    MUSIC_ZCR_MIN = 0.001    # Music th∆∞·ªùng c√≥ ZCR th·∫•p h∆°n
    MUSIC_ZCR_MAX = 0.05
    
    def __init__(self, device_index: Optional[int] = None):
        """
        Kh·ªüi t·∫°o AudioClassifier
        
        Args:
            device_index: Index c·ªßa ReSpeaker trong PyAudio (None = auto detect)
        """
        self.device_index = device_index
        self.p = pyaudio.PyAudio()
        self.stream = None
        self.is_recording = False
        
        if device_index is None:
            self.device_index = self._find_respeaker_device()

    def _find_respeaker_device(self) -> Optional[int]:
        """T·ª± ƒë·ªông t√¨m ReSpeaker device"""
        info = self.p.get_host_api_info_by_index(0)
        num_devices = info.get('deviceCount')
        
        for i in range(num_devices):
            device_info = self.p.get_device_info_by_host_api_device_index(0, i)
            if device_info.get('maxInputChannels') > 0:
                name = device_info.get('name')
                if 'ReSpeaker' in name or 'UAC1.0' in name:
                    print(f"‚úÖ T√¨m th·∫•y ReSpeaker: {name} (index: {i})")
                    return i
        
        print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y ReSpeaker, s·ª≠ d·ª•ng device m·∫∑c ƒë·ªãnh")
        return None

    def list_audio_devices(self):
        """Li·ªát k√™ t·∫•t c·∫£ audio devices"""
        print("\nüì¢ Danh s√°ch Audio Devices:")
        print("=" * 60)
        info = self.p.get_host_api_info_by_index(0)
        num_devices = info.get('deviceCount')
        
        for i in range(num_devices):
            device_info = self.p.get_device_info_by_host_api_device_index(0, i)
            if device_info.get('maxInputChannels') > 0:
                print(f"  [{i}] {device_info.get('name')}")
                print(f"      Channels: {device_info.get('maxInputChannels')}")
                print(f"      Sample Rate: {device_info.get('defaultSampleRate')}")
                print()

    def start_stream(self):
        """B·∫Øt ƒë·∫ßu audio stream"""
        try:
            self.stream = self.p.open(
                format=self.FORMAT,
                channels=self.CHANNELS,
                rate=self.RATE,
                input=True,
                input_device_index=self.device_index,
                frames_per_buffer=self.CHUNK
            )
            self.is_recording = True
            print("üé§ ƒê√£ b·∫Øt ƒë·∫ßu audio stream")
            return True
        except Exception as e:
            print(f"‚ùå L·ªói khi m·ªü stream: {e}")
            return False

    def stop_stream(self):
        """D·ª´ng audio stream"""
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
            self.is_recording = False
            print("‚èπÔ∏è  ƒê√£ d·ª´ng audio stream")

    def read_audio_chunk(self) -> Optional[np.ndarray]:
        """
        ƒê·ªçc m·ªôt chunk audio
        Returns: Numpy array ho·∫∑c None n·∫øu l·ªói
        """
        if not self.stream or not self.is_recording:
            return None
        
        try:
            data = self.stream.read(self.CHUNK, exception_on_overflow=False)
            # Convert bytes to numpy array
            audio_data = np.frombuffer(data, dtype=np.int16)
            
            # Extract channel 0 (processed audio for ASR)
            # Channel 0 l√† audio ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi c√°c thu·∫≠t to√°n tr√™n chip
            audio_channel_0 = audio_data[0::self.CHANNELS]
            
            return audio_channel_0
        except Exception as e:
            print(f"‚ùå L·ªói khi ƒë·ªçc audio: {e}")
            return None

    def calculate_rms(self, audio_data: np.ndarray) -> float:
        """
        T√≠nh RMS (Root Mean Square) - ƒëo ƒë·ªô l·ªõn √¢m thanh
        
        Args:
            audio_data: Numpy array c·ªßa audio
            
        Returns:
            RMS value
        """
        return np.sqrt(np.mean(audio_data.astype(float) ** 2))

    def calculate_zcr(self, audio_data: np.ndarray) -> float:
        """
        T√≠nh Zero-Crossing Rate - t·∫ßn su·∫•t ƒë·ªïi d·∫•u c·ªßa t√≠n hi·ªáu
        Speech: ZCR cao (nhi·ªÅu bi·∫øn ƒë·ªông)
        Music: ZCR th·∫•p h∆°n (m∆∞·ª£t h∆°n)
        
        Args:
            audio_data: Numpy array c·ªßa audio
            
        Returns:
            ZCR value (0-1)
        """
        signs = np.sign(audio_data)
        signs[signs == 0] = -1  # ƒê·ªïi 0 th√†nh -1
        zero_crossings = np.abs(np.diff(signs))
        zcr = np.sum(zero_crossings) / (2 * len(audio_data))
        return zcr

    def calculate_spectral_centroid(self, audio_data: np.ndarray) -> float:
        """
        T√≠nh Spectral Centroid - "tr·ªçng t√¢m" c·ªßa ph·ªï t·∫ßn s·ªë
        Speech: Centroid cao (nhi·ªÅu nƒÉng l∆∞·ª£ng ·ªü t·∫ßn s·ªë cao)
        Music: Ph√¢n b·ªë ƒë·ªÅu h∆°n
        
        Args:
            audio_data: Numpy array c·ªßa audio
            
        Returns:
            Spectral centroid (Hz)
        """
        # FFT ƒë·ªÉ l·∫•y ph·ªï t·∫ßn s·ªë
        spectrum = np.abs(np.fft.rfft(audio_data))
        freqs = np.fft.rfftfreq(len(audio_data), 1/self.RATE)
        
        # T√≠nh centroid
        if np.sum(spectrum) == 0:
            return 0
        
        centroid = np.sum(freqs * spectrum) / np.sum(spectrum)
        return centroid

    def extract_features(self, audio_data: np.ndarray) -> Dict:
        """
        Tr√≠ch xu·∫•t t·∫•t c·∫£ ƒë·∫∑c tr∆∞ng t·ª´ audio
        
        Args:
            audio_data: Numpy array c·ªßa audio
            
        Returns:
            Dictionary ch·ª©a c√°c features
        """
        if audio_data is None or len(audio_data) == 0:
            return {}
        
        return {
            'rms': self.calculate_rms(audio_data),
            'zcr': self.calculate_zcr(audio_data),
            'spectral_centroid': self.calculate_spectral_centroid(audio_data),
            'max_amplitude': np.max(np.abs(audio_data)),
            'mean_amplitude': np.mean(np.abs(audio_data))
        }

    def classify_sound(self, audio_data: np.ndarray) -> SoundType:
        """
        Ph√¢n lo·∫°i lo·∫°i √¢m thanh
        
        Args:
            audio_data: Numpy array c·ªßa audio
            
        Returns:
            SoundType enum
        """
        features = self.extract_features(audio_data)
        
        if not features:
            return SoundType.UNKNOWN
        
        rms = features['rms']
        zcr = features['zcr']
        
        # Rule 1: Silence detection
        if rms < self.SILENCE_THRESHOLD:
            return SoundType.SILENCE
        
        # Rule 2: Speech detection
        # Speech c√≥ ZCR trung b√¨nh, RMS dao ƒë·ªông
        if self.SPEECH_ZCR_MIN < zcr < self.SPEECH_ZCR_MAX:
            return SoundType.SPEECH
        
        # Rule 3: Music detection
        # Music c√≥ ZCR th·∫•p, m∆∞·ª£t m√† h∆°n
        if self.MUSIC_ZCR_MIN < zcr < self.MUSIC_ZCR_MAX and rms > self.SILENCE_THRESHOLD * 2:
            return SoundType.MUSIC
        
        # Rule 4: Noise
        # ZCR r·∫•t cao ho·∫∑c kh√¥ng ƒë·ªÅu
        if zcr > self.SPEECH_ZCR_MAX:
            return SoundType.NOISE
        
        return SoundType.UNKNOWN

    def classify_audio(self):
        """
        Ph√¢n lo·∫°i m·ªôt ƒëo·∫°n audio hi·ªán t·∫°i
        ƒê·ªçc chunk, ph√¢n t√≠ch v√† tr·∫£ v·ªÅ k·∫øt qu·∫£
        
        Returns:
            Tuple[SoundType, Dict]: (lo·∫°i √¢m thanh, features)
        """
        chunk = self.read_audio_chunk()
        if chunk is None:
            return (SoundType.UNKNOWN, {})
        
        features = self.extract_features(chunk)
        sound_type = self.classify_sound(chunk)
        
        return (sound_type, features)

    def analyze_continuous(self, duration: int = 10, interval: float = 0.5):
        """
        Ph√¢n t√≠ch li√™n t·ª•c trong kho·∫£ng th·ªùi gian
        
        Args:
            duration: Th·ªùi gian ph√¢n t√≠ch (gi√¢y)
            interval: Kho·∫£ng c√°ch gi·ªØa c√°c l·∫ßn ph√¢n t√≠ch (gi√¢y)
        """
        if not self.is_recording:
            print("‚ùå Stream ch∆∞a ƒë∆∞·ª£c m·ªü. G·ªçi start_stream() tr∆∞·ªõc.")
            return
        
        print(f"üéµ B·∫Øt ƒë·∫ßu ph√¢n t√≠ch √¢m thanh trong {duration} gi√¢y...")
        print("=" * 60)
        
        start_time = time.time()
        sound_counts = {st: 0 for st in SoundType}
        
        try:
            while time.time() - start_time < duration:
                # ƒê·ªçc nhi·ªÅu chunks ƒë·ªÉ c√≥ sample ƒë·ªß l·ªõn
                chunks = []
                for _ in range(int(interval * self.RATE / self.CHUNK)):
                    chunk = self.read_audio_chunk()
                    if chunk is not None:
                        chunks.append(chunk)
                
                if chunks:
                    audio_data = np.concatenate(chunks)
                    features = self.extract_features(audio_data)
                    sound_type = self.classify_sound(audio_data)
                    
                    sound_counts[sound_type] += 1
                    
                    # Emoji cho m·ªói lo·∫°i √¢m thanh
                    emoji_map = {
                        SoundType.SILENCE: "ü§´",
                        SoundType.SPEECH: "üó£Ô∏è",
                        SoundType.MUSIC: "üéµ",
                        SoundType.NOISE: "üîä",
                        SoundType.UNKNOWN: "‚ùì"
                    }
                    
                    print(f"{emoji_map[sound_type]} {sound_type.value.upper():8} | "
                          f"RMS: {features['rms']:7.0f} | "
                          f"ZCR: {features['zcr']:.4f} | "
                          f"Centroid: {features['spectral_centroid']:.0f} Hz")
                
                time.sleep(max(0, interval - (time.time() - start_time) % interval))
                
        except KeyboardInterrupt:
            print("\n‚èπÔ∏è  ƒê√£ d·ª´ng ph√¢n t√≠ch")
        
        # Th·ªëng k√™
        print("\n" + "=" * 60)
        print("üìä TH·ªêNG K√ä:")
        total = sum(sound_counts.values())
        for sound_type, count in sound_counts.items():
            if total > 0:
                percent = (count / total) * 100
                print(f"  {sound_type.value:8}: {count:3} l·∫ßn ({percent:5.1f}%)")

    def classify_continuous(self, duration: int = 10):
        """
        Ph√¢n lo·∫°i li√™n t·ª•c v√† tr·∫£ v·ªÅ k·∫øt qu·∫£
        T∆∞∆°ng th√≠ch v·ªõi CLI
        
        Args:
            duration: Th·ªùi gian ph√¢n lo·∫°i (gi√¢y)
            
        Returns:
            Dict[str, int]: S·ªë l·∫ßn ph√°t hi·ªán m·ªói lo·∫°i
        """
        if not self.start_stream():
            return {}
        
        sound_counts = {st.value: 0 for st in SoundType}
        start_time = time.time()
        
        try:
            while time.time() - start_time < duration:
                sound_type, _ = self.classify_audio()
                sound_counts[sound_type.value] += 1
                time.sleep(0.5)
        except KeyboardInterrupt:
            pass
        
        return sound_counts

    def record_to_file(self, filename: str, duration: int = 5):
        """
        Ghi √¢m v√† l∆∞u v√†o file WAV
        
        Args:
            filename: T√™n file output
            duration: Th·ªùi gian ghi (gi√¢y)
        """
        if not self.is_recording:
            print("‚ùå Stream ch∆∞a ƒë∆∞·ª£c m·ªü")
            return
        
        print(f"üî¥ ƒêang ghi √¢m {duration} gi√¢y...")
        frames = []
        
        for _ in range(0, int(self.RATE / self.CHUNK * duration)):
            data = self.stream.read(self.CHUNK, exception_on_overflow=False)
            frames.append(data)
        
        print(f"üíæ ƒêang l∆∞u v√†o {filename}...")
        
        wf = wave.open(filename, 'wb')
        wf.setnchannels(self.CHANNELS)
        wf.setsampwidth(self.p.get_sample_size(self.FORMAT))
        wf.setframerate(self.RATE)
        wf.writeframes(b''.join(frames))
        wf.close()
        
        print(f"‚úÖ ƒê√£ l∆∞u: {filename}")

    def cleanup(self):
        """D·ªçn d·∫πp resources"""
        self.stop_stream()
        self.p.terminate()
        print("üßπ ƒê√£ d·ªçn d·∫πp resources")
    
    def stop(self):
        """Alias for cleanup - for compatibility"""
        self.cleanup()


def main():
    """Demo s·ª≠ d·ª•ng AudioClassifier"""
    print("=" * 60)
    print("üéµ ReSpeaker Audio Classifier Demo")
    print("=" * 60)
    
    classifier = AudioClassifier()
    
    # Li·ªát k√™ devices
    classifier.list_audio_devices()
    
    # B·∫Øt ƒë·∫ßu stream
    if not classifier.start_stream():
        return
    
    try:
        # Ph√¢n t√≠ch li√™n t·ª•c
        classifier.analyze_continuous(duration=30, interval=0.5)
        
        # Ghi √¢m demo (optional)
        # classifier.record_to_file("test_recording.wav", duration=5)
        
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  ƒê√£ d·ª´ng")
    finally:
        classifier.cleanup()


if __name__ == '__main__':
    main()
