"""Audio Classifier Module for ReSpeaker"""

import pyaudio
import numpy as np
import wave
import time
from scipy import signal
from typing import Dict, List, Optional
from enum import Enum


class SoundType(Enum):
    SILENCE = "silence"
    SPEECH = "speech"
    MUSIC = "music"
    NOISE = "noise"
    UNKNOWN = "unknown"


class AudioClassifier:
    
    CHUNK = 1024
    RATE = 16000
    CHANNELS = 6
    FORMAT = pyaudio.paInt16
    
    SILENCE_THRESHOLD = 500
    SPEECH_ZCR_MIN = 0.01
    SPEECH_ZCR_MAX = 0.15
    MUSIC_ZCR_MIN = 0.001
    MUSIC_ZCR_MAX = 0.05
    
    def __init__(self, device_index: Optional[int] = None):
        self.device_index = device_index
        self.p = pyaudio.PyAudio()
        self.stream = None
        self.is_recording = False
        
        if device_index is None:
            self.device_index = self._find_respeaker_device()

    def _find_respeaker_device(self) -> Optional[int]:
        info = self.p.get_host_api_info_by_index(0)
        num_devices = info.get('deviceCount')
        
        for i in range(num_devices):
            device_info = self.p.get_device_info_by_host_api_device_index(0, i)
            if device_info.get('maxInputChannels') > 0:
                name = device_info.get('name')
                if 'ReSpeaker' in name or 'UAC1.0' in name:
                    print(f"‚úÖ T√¨m th·∫•y ReSpeaker: {name} (index: {i})")
                    return i
        
        print("Warning: ReSpeaker not found, using default device")
        return None

    def list_audio_devices(self):
        print("\nAudio Devices:")
        print("=" * 60)
        info = self.p.get_host_api_info_by_index(0)
        num_devices = info.get('deviceCount')
        
        for i in range(num_devices):
            device_info = self.p.get_device_info_by_host_api_device_index(0, i)
            if device_info.get('maxInputChannels') > 0:
                print(f"  [{i}] {device_info.get('name')}")
                print(f"      Channels: {device_info.get('maxInputChannels')}")
                print(f"      Sample Rate: {device_info.get('defaultSampleRate')}")
                print()

    def start_stream(self):
        try:
            self.stream = self.p.open(
                format=self.FORMAT,
                channels=self.CHANNELS,
                rate=self.RATE,
                input=True,
                input_device_index=self.device_index,
                frames_per_buffer=self.CHUNK
            )
            self.is_recording = True
            print("Audio stream started")
            return True
        except Exception as e:
            print(f"Error opening stream: {e}")
            return False

    def stop_stream(self):
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
            self.is_recording = False
            print("Audio stream stopped")

    def read_audio_chunk(self) -> Optional[np.ndarray]:
        if not self.stream or not self.is_recording:
            return None
        
        try:
            data = self.stream.read(self.CHUNK, exception_on_overflow=False)
            audio_data = np.frombuffer(data, dtype=np.int16)
            
            audio_channel_0 = audio_data[0::self.CHANNELS]
            
            return audio_channel_0
        except Exception as e:
            print(f"Error reading audio: {e}")
            return None

    def calculate_rms(self, audio_data: np.ndarray) -> float:
        return np.sqrt(np.mean(audio_data.astype(float) ** 2))

    def calculate_zcr(self, audio_data: np.ndarray) -> float:
        signs = np.sign(audio_data)
        signs[signs == 0] = -1
        zero_crossings = np.abs(np.diff(signs))
        zcr = np.sum(zero_crossings) / (2 * len(audio_data))
        return zcr

    def calculate_spectral_centroid(self, audio_data: np.ndarray) -> float:
        # FFT ƒë·ªÉ l·∫•y ph·ªï t·∫ßn s·ªë
        spectrum = np.abs(np.fft.rfft(audio_data))
        freqs = np.fft.rfftfreq(len(audio_data), 1/self.RATE)
        
        # T√≠nh centroid
        if np.sum(spectrum) == 0:
            return 0
        
        centroid = np.sum(freqs * spectrum) / np.sum(spectrum)
        return centroid

    def extract_features(self, audio_data: np.ndarray) -> Dict:
        """
        Tr√≠ch xu·∫•t t·∫•t c·∫£ ƒë·∫∑c tr∆∞ng t·ª´ audio
        
        Args:
            audio_data: Numpy array c·ªßa audio
            
        Returns:
            Dictionary ch·ª©a c√°c features
        """
        if audio_data is None or len(audio_data) == 0:
            return {}
        
        return {
            'rms': self.calculate_rms(audio_data),
            'zcr': self.calculate_zcr(audio_data),
            'spectral_centroid': self.calculate_spectral_centroid(audio_data),
            'max_amplitude': np.max(np.abs(audio_data)),
            'mean_amplitude': np.mean(np.abs(audio_data))
        }

    def classify_sound(self, audio_data: np.ndarray) -> SoundType:
        """
        Ph√¢n lo·∫°i lo·∫°i √¢m thanh
        
        Args:
            audio_data: Numpy array c·ªßa audio
            
        Returns:
            SoundType enum
        """
        features = self.extract_features(audio_data)
        
        if not features:
            return SoundType.UNKNOWN
        
        rms = features['rms']
        zcr = features['zcr']
        
        # Rule 1: Silence detection
        if rms < self.SILENCE_THRESHOLD:
            return SoundType.SILENCE
        
        # Rule 2: Speech detection
        # Speech c√≥ ZCR trung b√¨nh, RMS dao ƒë·ªông
        if self.SPEECH_ZCR_MIN < zcr < self.SPEECH_ZCR_MAX:
            return SoundType.SPEECH
        
        # Rule 3: Music detection
        # Music c√≥ ZCR th·∫•p, m∆∞·ª£t m√† h∆°n
        if self.MUSIC_ZCR_MIN < zcr < self.MUSIC_ZCR_MAX and rms > self.SILENCE_THRESHOLD * 2:
            return SoundType.MUSIC
        
        # Rule 4: Noise
        # ZCR r·∫•t cao ho·∫∑c kh√¥ng ƒë·ªÅu
        if zcr > self.SPEECH_ZCR_MAX:
            return SoundType.NOISE
        
        return SoundType.UNKNOWN

    def classify_audio(self):
        chunk = self.read_audio_chunk()
        if chunk is None:
            return (SoundType.UNKNOWN, {})
        
        features = self.extract_features(chunk)
        sound_type = self.classify_sound(chunk)
        
        return (sound_type, features)

    def analyze_continuous(self, duration: int = 10, interval: float = 0.5):
        if not self.is_recording:
            print("Stream not opened. Call start_stream() first.")
            return
        
        print(f"Analyzing audio for {duration} seconds...")
        print("=" * 60)
        
        start_time = time.time()
        sound_counts = {st: 0 for st in SoundType}
        
        try:
            while time.time() - start_time < duration:
                chunks = []
                for _ in range(int(interval * self.RATE / self.CHUNK)):
                    chunk = self.read_audio_chunk()
                    if chunk is not None:
                        chunks.append(chunk)
                
                if chunks:
                    audio_data = np.concatenate(chunks)
                    features = self.extract_features(audio_data)
                    sound_type = self.classify_sound(audio_data)
                    
                    sound_counts[sound_type] += 1
                    
                    emoji_map = {
                        SoundType.SILENCE: "ü§´",
                        SoundType.SPEECH: "üó£Ô∏è",
                        SoundType.MUSIC: "üéµ",
                        SoundType.NOISE: "üîä",
                        SoundType.UNKNOWN: "‚ùì"
                    }
                    
                    print(f"{emoji_map[sound_type]} {sound_type.value.upper():8} | "
                          f"RMS: {features['rms']:7.0f} | "
                          f"ZCR: {features['zcr']:.4f} | "
                          f"Centroid: {features['spectral_centroid']:.0f} Hz")
                
                time.sleep(max(0, interval - (time.time() - start_time) % interval))
                
        except KeyboardInterrupt:
            print("\n‚èπÔ∏è  ƒê√£ d·ª´ng ph√¢n t√≠ch")
        
        # Th·ªëng k√™
        print("\n" + "=" * 60)
        print("üìä TH·ªêNG K√ä:")
        total = sum(sound_counts.values())
        for sound_type, count in sound_counts.items():
            if total > 0:
                percent = (count / total) * 100
                print(f"  {sound_type.value:8}: {count:3} l·∫ßn ({percent:5.1f}%)")

    def classify_continuous(self, duration: int = 10):
        """
        Ph√¢n lo·∫°i li√™n t·ª•c v√† tr·∫£ v·ªÅ k·∫øt qu·∫£
        T∆∞∆°ng th√≠ch v·ªõi CLI
        
        Args:
            duration: Th·ªùi gian ph√¢n lo·∫°i (gi√¢y)
            
        Returns:
            Dict[str, int]: S·ªë l·∫ßn ph√°t hi·ªán m·ªói lo·∫°i
        """
        if not self.start_stream():
            return {}
        
        sound_counts = {st.value: 0 for st in SoundType}
        start_time = time.time()
        
        try:
            while time.time() - start_time < duration:
                sound_type, _ = self.classify_audio()
                sound_counts[sound_type.value] += 1
                time.sleep(0.5)
        except KeyboardInterrupt:
            pass
        
        return sound_counts

    def record_to_file(self, filename: str, duration: int = 5):
        """
        Ghi √¢m v√† l∆∞u v√†o file WAV
        
        Args:
            filename: T√™n file output
            duration: Th·ªùi gian ghi (gi√¢y)
        """
        if not self.is_recording:
            print("‚ùå Stream ch∆∞a ƒë∆∞·ª£c m·ªü")
            return
        
        print(f"üî¥ ƒêang ghi √¢m {duration} gi√¢y...")
        frames = []
        
        for _ in range(0, int(self.RATE / self.CHUNK * duration)):
            data = self.stream.read(self.CHUNK, exception_on_overflow=False)
            frames.append(data)
        
        print(f"üíæ ƒêang l∆∞u v√†o {filename}...")
        
        wf = wave.open(filename, 'wb')
        wf.setnchannels(self.CHANNELS)
        wf.setsampwidth(self.p.get_sample_size(self.FORMAT))
        wf.setframerate(self.RATE)
        wf.writeframes(b''.join(frames))
        wf.close()
        
        print(f"‚úÖ ƒê√£ l∆∞u: {filename}")

    def cleanup(self):
        """D·ªçn d·∫πp resources"""
        self.stop_stream()
        self.p.terminate()
        print("üßπ ƒê√£ d·ªçn d·∫πp resources")
    
    def stop(self):
        """Alias for cleanup - for compatibility"""
        self.cleanup()


def main():
    """Demo s·ª≠ d·ª•ng AudioClassifier"""
    print("=" * 60)
    print("üéµ ReSpeaker Audio Classifier Demo")
    print("=" * 60)
    
    classifier = AudioClassifier()
    
    # Li·ªát k√™ devices
    classifier.list_audio_devices()
    
    # B·∫Øt ƒë·∫ßu stream
    if not classifier.start_stream():
        return
    
    try:
        # Ph√¢n t√≠ch li√™n t·ª•c
        classifier.analyze_continuous(duration=30, interval=0.5)
        
        # Ghi √¢m demo (optional)
        # classifier.record_to_file("test_recording.wav", duration=5)
        
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  ƒê√£ d·ª´ng")
    finally:
        classifier.cleanup()


if __name__ == '__main__':
    main()
